# Attention Is All You Need

This repo is a work-in-progress towards the goal of a minimal implementation of [transformers with multi-head self attention](https://arxiv.org/abs/1706.03762), for my own curiosity and deeper understanding.  
